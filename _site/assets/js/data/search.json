[
  
  {
    "title": "Building Production-Ready RAG Systems with LangChain",
    "url": "/posts/building-rag-systems-with-langchain/",
    "categories": "AI, Tutorial",
    "tags": "rag, langchain, python, vector-database, llm",
    "date": "2025-12-11 14:00:00 +1000",
    "content": "Introduction  Retrieval-Augmented Generation (RAG) has become the go-to pattern for building AI applications that need access to custom knowledge bases. In this post, weâ€™ll build a production-ready RAG system from scratch.     This is a technical template post demonstrating Chirpyâ€™s features. Replace the content with your actual tutorial!   What is RAG?  RAG combines the power of large language models with external knowledge retrieval. Instead of relying solely on the LLMâ€™s training data, RAG:     Retrieves relevant documents from a knowledge base   Augments the prompt with this context   Generates a response grounded in the retrieved information   flowchart LR     A[User Query] --&gt; B[Embedding Model]     B --&gt; C[Vector Search]     C --&gt; D[Retrieved Documents]     D --&gt; E[LLM + Context]     E --&gt; F[Response]   Prerequisites  Before we start, ensure you have:     Python 3.9+   OpenAI API key (or another LLM provider)   Basic understanding of embeddings and vector databases   pip install langchain langchain-openai chromadb tiktoken   Project Structure  rag-system/ â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ __init__.py â”‚   â”œâ”€â”€ embeddings.py â”‚   â”œâ”€â”€ retriever.py â”‚   â”œâ”€â”€ chain.py â”‚   â””â”€â”€ main.py â”œâ”€â”€ data/ â”‚   â””â”€â”€ documents/ â”œâ”€â”€ tests/ â”‚   â””â”€â”€ test_retriever.py â”œâ”€â”€ .env â””â”€â”€ requirements.txt   Step 1: Document Loading and Chunking  The first step is loading and chunking your documents:  from langchain.document_loaders import DirectoryLoader, TextLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  def load_documents(directory: str) -&gt; list:     \"\"\"Load documents from a directory.\"\"\"     loader = DirectoryLoader(         directory,         glob=\"**/*.txt\",         loader_cls=TextLoader     )     documents = loader.load()      # Chunk documents for better retrieval     text_splitter = RecursiveCharacterTextSplitter(         chunk_size=1000,         chunk_overlap=200,         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]     )      chunks = text_splitter.split_documents(documents)     return chunks      Pro Tip: Chunk size significantly impacts retrieval quality. Smaller chunks (500-1000 tokens) often work better for specific queries, while larger chunks preserve more context.   Step 2: Creating the Vector Store  from langchain_openai import OpenAIEmbeddings from langchain.vectorstores import Chroma  def create_vector_store(documents: list, persist_directory: str) -&gt; Chroma:     \"\"\"Create and persist a vector store.\"\"\"     embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")      vector_store = Chroma.from_documents(         documents=documents,         embedding=embeddings,         persist_directory=persist_directory     )      return vector_store   Step 3: Building the RAG Chain  from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser  def create_rag_chain(retriever):     \"\"\"Create a RAG chain with the given retriever.\"\"\"      template = \"\"\"Answer the question based on the following context.     If you cannot answer from the context, say so.      Context:     {context}      Question: {question}      Answer:\"\"\"      prompt = ChatPromptTemplate.from_template(template)     llm = ChatOpenAI(model=\"gpt-4\", temperature=0)      chain = (         {\"context\": retriever, \"question\": RunnablePassthrough()}         | prompt         | llm         | StrOutputParser()     )      return chain   Step 4: Putting It All Together  def main():     # Load and process documents     documents = load_documents(\"./data/documents\")      # Create vector store     vector_store = create_vector_store(documents, \"./chroma_db\")      # Create retriever with search parameters     retriever = vector_store.as_retriever(         search_type=\"similarity\",         search_kwargs={\"k\": 4}     )      # Create RAG chain     chain = create_rag_chain(retriever)      # Query the system     response = chain.invoke(\"What is the main topic of the documents?\")     print(response)  if __name__ == \"__main__\":     main()   Advanced: Hybrid Search  For better results, combine semantic and keyword search:  from langchain.retrievers import BM25Retriever, EnsembleRetriever  def create_hybrid_retriever(documents, vector_store):     \"\"\"Create a hybrid retriever combining BM25 and vector search.\"\"\"      # Keyword-based retriever     bm25_retriever = BM25Retriever.from_documents(documents)     bm25_retriever.k = 4      # Vector-based retriever     vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})      # Combine with weighted ensemble     ensemble_retriever = EnsembleRetriever(         retrievers=[bm25_retriever, vector_retriever],         weights=[0.3, 0.7]  # Favor semantic search     )      return ensemble_retriever   Performance Considerations                 Approach       Latency       Accuracy       Cost                       Simple RAG       Low       Medium       $                 Hybrid Search       Medium       High       $$                 Reranking       High       Very High       $$$           Common Pitfalls     Warning: Avoid these common mistakes when building RAG systems:      Chunks too large â€” Retrieved context may contain irrelevant information   No overlap â€” Important context split across chunk boundaries   Poor prompting â€” Not instructing the LLM to use the context properly   Ignoring metadata â€” Useful for filtering and attribution   Next Steps  In future posts, weâ€™ll cover:     Adding reranking with Cohere or cross-encoders   Implementing conversation memory   Evaluating RAG performance with RAGAS   Deploying with FastAPI   Resources     LangChain Documentation   Chroma Vector Database   OpenAI Embeddings Guide     Have questions or suggestions? Reach out on Twitter or leave a comment below!"
  },
  
  {
    "title": "Welcome to willburnstech - My Journey into AI, Security & Full-Stack Dev",
    "url": "/posts/welcome-to-my-blog/",
    "categories": "General, Welcome",
    "tags": "introduction, ai, security, full-stack",
    "date": "2025-12-11 10:00:00 +1000",
    "content": "Hello World! ðŸš€  Welcome to willburnstech â€” my corner of the internet where I document my journey through the ever-evolving landscape of technology.  Iâ€™m Will, a Full-Stack AI Engineer, Computer Science student, and founder of AutomateWise. This blog is where Iâ€™ll be sharing:     AI Engineering â€” Practical implementations, LLM patterns, and automation workflows   Security Research â€” Penetration testing insights, CTF writeups, and vulnerability analysis   Full-Stack Development â€” Building modern web applications with Next.js, Python, and more   Why Start a Blog?  In the AI revolution weâ€™re living through, I believe itâ€™s more important than ever to:     Document and share knowledge â€” The best way to learn is to teach   Build in public â€” Show the process, not just the polished results   Connect with the community â€” Find like-minded builders and security researchers   What to Expect  Technical Deep-Dives  Iâ€™ll be writing detailed tutorials and breakdowns on topics like:     Building RAG systems with LangChain and vector databases   Setting up AI agents with tool use and memory   Security testing methodologies and tools   Full-stack architecture patterns   Project Walkthroughs  Expect posts covering real projects Iâ€™m working on:     TaxGuru AI â€” An AI research assistant for accountants   AutomateWise â€” AI chatbots, voice agents, and workflow automations   Various security research and CTF challenges   Code &amp; Examples  Every technical post will include working code examples. Hereâ€™s a taste of whatâ€™s to come:  # A simple example of what we'll be building from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate  llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  prompt = ChatPromptTemplate.from_messages([     (\"system\", \"You are a helpful AI assistant.\"),     (\"user\", \"{input}\") ])  chain = prompt | llm response = chain.invoke({\"input\": \"What can you help me with?\"})   Get in Touch  Iâ€™d love to connect with fellow developers, security researchers, and AI enthusiasts:     GitHub: @willburnstech   Twitter/X: @willburnstech   LinkedIn: /in/willburnstech   Letâ€™s Build Together  Stay tuned for more content. If thereâ€™s a specific topic youâ€™d like me to cover, reach out on social media!  â€” Will"
  }
  
]

